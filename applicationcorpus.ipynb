{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD NYAYANUMANA_KNOWLEDGE.CSV\n"
      ],
      "metadata": {
        "id": "0Fp3Wjsaapd8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZ5_M-i7ap4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET TOP X DATAPOINTS FROM ORIGINAL DATASETN OP : CSV OF GIVEN NO OF ROWS"
      ],
      "metadata": {
        "id": "v4kw8X_qXn5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRm3pUB8XMv8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the input CSV file\n",
        "input_csv_path = \"/content/nyayanumana_knowledge.csv\"\n",
        "\n",
        "# Define the path for the output CSV file\n",
        "output_csv_path = \"selected_datapoints.csv\"\n",
        "\n",
        "# Define the start and end row (inclusive) for extraction\n",
        "start_row = 0 # Remember that pandas indexing starts from 0\n",
        "end_row = 199  # To select the first 200 rows (0 to 199)\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    # Use engine='python' and on_bad_lines='skip' to handle potential parsing errors\n",
        "    df = pd.read_csv(input_csv_path, engine='python', on_bad_lines='skip')\n",
        "    print(f\"Successfully read {input_csv_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {input_csv_path} was not found.\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV: {e}\")\n",
        "    df = None\n",
        "\n",
        "\n",
        "# Check if the DataFrame was loaded and has the specified rows\n",
        "if df is not None:\n",
        "    # Ensure end_row is within the DataFrame bounds\n",
        "    if end_row >= len(df):\n",
        "        print(f\"Warning: end_row ({end_row}) is beyond the number of rows in the DataFrame ({len(df)}). Extracting up to the last row.\")\n",
        "        end_row = len(df) - 1\n",
        "    if start_row < 0:\n",
        "        print(f\"Warning: start_row ({start_row}) is less than 0. Starting extraction from row 0.\")\n",
        "        start_row = 0\n",
        "    if start_row > end_row:\n",
        "        print(f\"Error: start_row ({start_row}) is greater than end_row ({end_row}). No rows will be extracted.\")\n",
        "        selected_df = pd.DataFrame() # Create an empty DataFrame\n",
        "    else:\n",
        "        # Select the specified range of rows\n",
        "        selected_df = df.iloc[start_row : end_row + 1] # Add 1 to end_row to make it inclusive\n",
        "        print(f\"Selected rows from {start_row} to {end_row}.\")\n",
        "\n",
        "\n",
        "    # Save the selected data points to a new CSV file\n",
        "    if not selected_df.empty:\n",
        "        try:\n",
        "            # Set index=False to avoid writing the DataFrame index as a column in the CSV\n",
        "            selected_df.to_csv(output_csv_path, index=False)\n",
        "            print(f\"Successfully saved the selected data points to {output_csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving the DataFrame to CSV: {e}\")\n",
        "    else:\n",
        "        print(\"No data points selected to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "API KEYS"
      ],
      "metadata": {
        "id": "K3Kfkcr-XwpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace with your actual API keys\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"763be8ff039562b47f718f61a7e7c05eb36e834004a995dcd964dd219b2cc37b\"\n",
        "\n",
        "print(\"Environment variables GROQ_API_KEY and SERPAPI_API_KEY have been set.\")"
      ],
      "metadata": {
        "id": "wy5F5nJTXyh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLATIONS"
      ],
      "metadata": {
        "id": "HSFt5ZSlX1KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SerpApi\n",
        "!pip install groq\n",
        "!pip install google-search-results\n",
        "!pip install google_search_results"
      ],
      "metadata": {
        "id": "XRoEFFfJX2Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXTRACTING 'text' FIELD FROM THE CSV OP : EXTRACTED TEXT.JSON FILE CONTATINING TEXT COLUMNS OF THE CSV"
      ],
      "metadata": {
        "id": "pZ-caLFwX-as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import ast\n",
        "import json\n",
        "\n",
        "def extract_text_from_csv(input_csv_path, output_json_path):\n",
        "    \"\"\"\n",
        "    Extracts the 'text' field from each row of a CSV file and saves it\n",
        "    as a list of dictionaries in a JSON file.\n",
        "\n",
        "    The script assumes the text data is in the first column of the CSV,\n",
        "    formatted as a string representation of a Python dictionary.\n",
        "\n",
        "    Args:\n",
        "        input_csv_path (str): The path to the input CSV file.\n",
        "        output_json_path (str): The path for the output JSON file.\n",
        "    \"\"\"\n",
        "    extracted_data = []\n",
        "    try:\n",
        "        with open(input_csv_path, 'r', encoding='utf-8') as csvfile:\n",
        "            csv_reader = csv.reader(csvfile)\n",
        "\n",
        "            # Skip the header row\n",
        "            next(csv_reader, None)\n",
        "\n",
        "            for row in csv_reader:\n",
        "                # Ensure the row is not empty\n",
        "                if not row:\n",
        "                    continue\n",
        "\n",
        "                # The dictionary-like string is in the first column\n",
        "                data_string = row[0]\n",
        "\n",
        "                try:\n",
        "                    # Safely evaluate the string to a Python dictionary\n",
        "                    data_dict = ast.literal_eval(data_string)\n",
        "\n",
        "                    # Extract the value from the 'text' key and append\n",
        "                    if isinstance(data_dict, dict) and 'text' in data_dict:\n",
        "                        extracted_data.append({'text': data_dict['text']})\n",
        "                except (ValueError, SyntaxError) as e:\n",
        "                    print(f\"Could not parse row: {data_string[:100]}... Error: {e}\")\n",
        "\n",
        "        # Write the list of dictionaries to the output JSON file\n",
        "        with open(output_json_path, 'w', encoding='utf-8') as jsonfile:\n",
        "            json.dump(extracted_data, jsonfile, indent=4)\n",
        "\n",
        "        print(f\"Successfully extracted data to '{output_json_path}'\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{input_csv_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the input and output file names\n",
        "    input_file = '/content/top_300_datapoints.csv'\n",
        "    output_file = 'extracted_data.json'\n",
        "\n",
        "    # Run the extraction function\n",
        "    extract_text_from_csv(input_file, output_file)"
      ],
      "metadata": {
        "id": "DO-eZTRjYGtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE FOR BUILDING APPLICATION CORPUS\n",
        "OP : APPLICATION CORPUS.CSV"
      ],
      "metadata": {
        "id": "-FFh0q40YPkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from groq import Groq\n",
        "from serpapi import GoogleSearch\n",
        "import os\n",
        "import json # Import the json library\n",
        "\n",
        "# Replace with your actual API keys\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"763be8ff039562b47f718f61a7e7c05eb36e834004a995dcd964dd219b2cc37b\"\n",
        "\n",
        "print(\"Environment variables GROQ_API_KEY and SERPAPI_API_KEY have been set.\")\n",
        "\n",
        "# ---------------- Configuration ----------------\n",
        "GROQ_API_KEY =\"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\"\n",
        "SERPAPI_KEY = \"763be8ff039562b47f718f61a7e7c05eb36e834004a995dcd964dd219b2cc37b\"\n",
        "\n",
        "OUTPUT_CSV_FILE = \"application_corpus.csv\"\n",
        "\n",
        "# Allowed legal source domains for filtering\n",
        "LEGAL_DOMAINS = [\n",
        "    \"indiankanoon.org\",\n",
        "    \"barandbench.com\",\n",
        "    \"livelaw.in\",\n",
        "    \"scobserver.in\",\n",
        "    \"legalserviceindia.com\",\n",
        "]\n",
        "\n",
        "# ---------------- Helper: Check API Keys ----------------\n",
        "def check_api_keys():\n",
        "    if not GROQ_API_KEY:\n",
        "        print(\"ERROR: GROQ_API_KEY not set.\")\n",
        "        sys.exit(1)\n",
        "    if not SERPAPI_KEY:\n",
        "        print(\"ERROR: SERPAPI_KEY not set.\")\n",
        "        sys.exit(1)\n",
        "    print(\"API keys verified.\\n\")\n",
        "\n",
        "\n",
        "# ---------------- LLM Call Wrapper ----------------\n",
        "def llm_call(prompt, task_description):\n",
        "    try:\n",
        "        print(f\"\\n[LLM] {task_description} ...\")\n",
        "        client = Groq(api_key=GROQ_API_KEY)\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Groq API error during '{task_description}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# ---------------- 1️⃣ Generate Search Query ----------------\n",
        "def generate_search_query(original_text):\n",
        "    prompt = f\"\"\"\n",
        "You are generating a legal search query based on this legal context.\n",
        "\n",
        "Context:\n",
        "{original_text}\n",
        "\n",
        "Generate a short, specific Google search query that would return Indian court judgments or legal discussions related to this context.\n",
        "\"\"\"\n",
        "    return llm_call(prompt, \"Generate legal search query\")\n",
        "\n",
        "\n",
        "# ---------------- 2️⃣ Get Top Search Links (Modified) ----------------\n",
        "def get_top_search_links(query, top_k=3):\n",
        "    print(f\"\\n🔍 Searching for: {query}\")\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"num\": top_k * 3,  # retrieve extra to filter later\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    all_results = results.get(\"organic_results\", [])\n",
        "    if not all_results:\n",
        "        print(\"No search results found.\")\n",
        "        return []\n",
        "\n",
        "    filtered_links = []\n",
        "    for result in all_results:\n",
        "        link = result.get(\"link\", \"\")\n",
        "        if any(domain in link for domain in LEGAL_DOMAINS):\n",
        "            filtered_links.append(link)\n",
        "        if len(filtered_links) >= top_k:\n",
        "            break\n",
        "\n",
        "    print(f\"✅ Retrieved {len(filtered_links)} valid legal links.\")\n",
        "    return filtered_links\n",
        "\n",
        "\n",
        "# ---------------- 3️⃣ Scrape and Clean Page ----------------\n",
        "def get_and_clean_page_content(url):\n",
        "    try:\n",
        "        print(f\"Scraping: {url}\")\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        res = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        # Extract main content\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        cleaned_text = \" \".join(text.split())\n",
        "        return cleaned_text[:4000]  # keep it concise\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# ---------------- 4️⃣ Summarize Content for Application ----------------\n",
        "def summarize_content_for_application(retrieved_text, original_text):\n",
        "    prompt = f\"\"\"\n",
        "You are creating a legal 'application corpus' example.\n",
        "\n",
        "Base legal principle:\n",
        "{original_text}\n",
        "\n",
        "Retrieved real-world legal case content:\n",
        "{retrieved_text}\n",
        "\n",
        "Summarize how the retrieved case applies or exemplifies the base principle.\n",
        "Keep it concise (max 6 sentences).\n",
        "\"\"\"\n",
        "    return llm_call(prompt, \"Summarize retrieved legal content\")\n",
        "\n",
        "\n",
        "# ---------------- 5️⃣ Generate Final Reasoning ----------------\n",
        "def generate_final_reasoning(original_text, summary):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI trained in Indian legal reasoning.\n",
        "\n",
        "Base Principle:\n",
        "{original_text}\n",
        "\n",
        "Application Summary:\n",
        "{summary}\n",
        "\n",
        "Generate a brief reasoning statement connecting the base principle to the summarized application.\n",
        "Focus on cause-effect legal reasoning.\n",
        "\"\"\"\n",
        "    return llm_call(prompt, \"Generate final reasoning\")\n",
        "\n",
        "\n",
        "# ---------------- 6️⃣ Main Pipeline ----------------\n",
        "def build_application_corpus(dataset):\n",
        "    with open(OUTPUT_CSV_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Base Text\", \"Sources\", \"Retrieved Summary\", \"Final Reasoning\"])\n",
        "\n",
        "        for i, data in enumerate(dataset, start=1):\n",
        "            print(f\"\\n============================\")\n",
        "            print(f\"Processing #{i}\")\n",
        "            print(\"============================\")\n",
        "\n",
        "            original_text = data[\"text\"]\n",
        "\n",
        "            # Step 1: Generate search query\n",
        "            query = generate_search_query(original_text)\n",
        "\n",
        "            # Step 2: Retrieve top links\n",
        "            links = get_top_search_links(query, top_k=3)\n",
        "            if not links:\n",
        "                writer.writerow([original_text, \"\", \"\", \"\"])\n",
        "                continue\n",
        "\n",
        "            # Step 3: Combine multiple page contents\n",
        "            combined_text = \"\"\n",
        "            for link in links:\n",
        "                page_text = get_and_clean_page_content(link)\n",
        "                if page_text:\n",
        "                    combined_text += f\"\\n\\nSource: {link}\\n{page_text}\"\n",
        "\n",
        "            # Step 4: Summarize\n",
        "            summary = summarize_content_for_application(combined_text, original_text)\n",
        "\n",
        "            # Step 5: Generate reasoning\n",
        "            reasoning = generate_final_reasoning(original_text, summary)\n",
        "\n",
        "            # Step 6: Save in CSV\n",
        "            writer.writerow([original_text, \"; \".join(links), summary, reasoning])\n",
        "            print(\"✅ Entry added to corpus.\\n\")\n",
        "\n",
        "            # Delay between entries to avoid SerpApi rate limit\n",
        "            time.sleep(3)\n",
        "\n",
        "\n",
        "# ---------------- Execute ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    check_api_keys()\n",
        "    # Load data from the JSON file\n",
        "    json_file_path = \"/content/extracted_data.json\"\n",
        "    try:\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            nyayanumana_dataset = json.load(f)\n",
        "        print(f\"Successfully loaded data from {json_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {json_file_path} was not found.\")\n",
        "        nyayanumana_dataset = []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {json_file_path}. Please ensure the file is valid JSON.\")\n",
        "        nyayanumana_dataset = []\n",
        "\n",
        "\n",
        "    build_application_corpus(nyayanumana_dataset)\n",
        "    print(f\"\\n✅ Application corpus created: {OUTPUT_CSV_FILE}\")"
      ],
      "metadata": {
        "id": "Apare6h5YSi9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}